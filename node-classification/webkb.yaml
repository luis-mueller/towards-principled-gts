# Training
batch_size: 64
lr: 5e-4
weight_decay: 1e-5
gradient_norm: 1.0

# Transformer
num_layers: 2
embed_dim: 256
num_heads: 8
activation: relu
attention_dropout: 0.2
ffn_dropout: 0.2

# Misc
seed: 0
fold: 0
dataset: Cornell
wandb_project: null
wandb_entity: null
wandb_name: null
root: ???

