# Training
batch_size: 32
lr: 1e-3
weight_decay: 1e-5
gradient_norm: 1.0

# Edge Transformer
num_layers: 6
embed_dim: 64
num_heads: 8
activation: gelu
pooling: sum
attention_dropout: 0.2
ffn_dropout: 0.0

# Encoder
rrwp: no

# Misc
seed: 0
wandb_project: null
wandb_entity: null
wandb_name: null
root: ???