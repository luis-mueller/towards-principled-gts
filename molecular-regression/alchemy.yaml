# Training
batch_size: 64
lr: 1e-3
weight_decay: 1e-5
gradient_norm: 1.0

# Transformer
num_layers: 10
embed_dim: 64
num_heads: 8
activation: gelu
pooling: sum
attention_dropout: 0.2
ffn_dropout: 0.0

# Encoder
rrwp: no

# Misc
compiled: no
dtype: float32
seed: 0
wandb_project: null
wandb_entity: null
wandb_name: null
root: ???
