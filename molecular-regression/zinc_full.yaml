# Training
batch_size: 256
lr: 1e-3
weight_decay: 1e-5
gradient_norm: 1.0
num_epochs: 1000

# Edge Transformer
num_layers: 10
embed_dim: 64
num_heads: 8
activation: gelu
pooling: sum
attention_dropout: 0.2
ffn_dropout: 0.0
num_workers: 8

# Encoder
rrwp: no

# Misc
checkpoint: null
compiled: True
dtype: float32
seed: 0
wandb_project: null
wandb_entity: null
wandb_name: null
root: ???